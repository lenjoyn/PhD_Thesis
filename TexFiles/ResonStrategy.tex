\chapter{Resonance Searching Strategy}
\chapterquote{The wilderness must be explored!}{Russell, Up}
The search for resonance signatures is applying a general strategy with three benchmarks for exotic particles of different spins: the narrow-width-approximation scalar boson (NWA, spin=0), the heavy vector triplet (HVT W', Z' bosons, spin=1), and the Randall-Sundrum graviton (RSG, spin=2).
\\
\\In the study of this thesis, $WW$ and $WZ$ are the two medium states of interest through the production of gluon-gluon fusion (ggF), Drell-Yan process (DY), or vector boson fusion (VBF). The vector boson fusion is the fusion process of two vector bosons ($W$ or $Z$) emitted from two incoming quarks, and the two quarks are then scattered into two energetic jets with a wide $\eta$ separation and high invariant mass, which is taken as the key signature to select VBF events (details in event selected section). The production processes could be seen in Fig. \ref{Fig:Xprod} as Feynman diagrams, and it should be noted that DY and ggF are indistinguishable from detector signatures. The strategy herein considers only final states in which one $W$ boson decays leptonically ($W\rightarrow l\nu$) into an electron or muon accompanied by a neutrino of the corresponding flavour, while the other boson,$W$ or $Z$, is chosen to decay hadronically into two quarks reconstructed into two $R=0.4$ jets or one $R=1.0$ jet ($W/Z\rightarrow jj$ or $W/Z\rightarrow J$). The events with W bosons decayed into taus are not considered in this analysis. The benefit of choosing this final state is to have the high branching ratio from the hadronic decay and suppress the QCD contamination by the leptonic decay. This study is conducted to search for particles in a wide mass range from $300~GeV$ to $5~TeV$. If the mass of a resonance particle is high enough ($m>1~TeV$), the bosons would be highly boosted. In this scenario, the two quarks decayed from boosted bosons get too close to each other which is beyond the resolution of anti-$k_{T}$ algorithm with $R=0.4$, so they are not resolved as two jets, and a larger cone of $R=1.0$ is applied to collect their signatures into a single fat jet. 
\\
\\This search was performed with the $36.1fb^{-1}$ data collected by the ATLAS detector in 2015 and 2016 with pp collisions at $\sqrt{s}=13~TeV$ (s is a Mandelstam variable meaning the square energy sum of the two quarks involved in a collision). 

\begin{figure}[htp]
	\centering
	\subfloat[Drell-Yan Process]{\includegraphics[width=.3\textwidth]{Chapter3/DY_X.pdf}}\hfill
	\subfloat[gluon-gluon Fusion]{\includegraphics[width=.3\textwidth]{Chapter3/ggF_X.pdf}}\hfill
	\subfloat[Vector Boson Fusion]{\includegraphics[width=.3\textwidth]{Chapter3/VBF_X.pdf}}
	\caption{The Feynman diagrams of different production mechanisms for particle X which decays into two SM bosons.}
	\label{Fig:Xprod}
\end{figure}
\section{Signal Models}
\label{sec:signal_intro}
In the SM, bosons are the force carriers and also maintain the conservation of certain physical quantities associated with underlying symmetries. To seek for the solution of unsolved problems of the SM, many new models predict the existence of new bosons corresponding to unknown interactions or symmetries, and they also have the strong coupling to the SM bosons which provide the access to verify those theories. However, the existing new models are constructed with many free parameters, and each set of them needs a dedicated analysis from the experimental side, which is impossible in reality. Therefore, a simplified model with only the kinematic parameters related to resonance mass is introduced for which experiments provide precise measurements for on-shell bosons.  
\\
\\This strategy could scan through many models, so it is defined as a general search. However, to give a better separation between signal and background, three benchmarks are applied in this analysis for sensitivity optimization as mentioned before. 
\\
\\{\bf Narrow-Width-Approximation Higgs Boson}
\\
\\Some extended models predict the existence of high mass Higgs bosons (BSM Higgs boson) to solve the problems of Higgs boson naturalness. However, as only kinematic properties are concerned, the interpretation model chosen in this analysis is the SM Higgs boson but with higher mass. To have further simplification, the decay of the Higgs boson is forced to be always at the mass pole with the narrow width approximation. This means the transferred momentum, $q$, from the proton partons is exactly the mass of the resonant particle under the assumption, which gives the narrow resonance width, $\Gamma/m_{H}<<1$ ($\Gamma$ is the mass width on Higgs boson mass spectrum with $m_{H}$ as the new mass of Higgs boson), and the interference to the SM Higgs boson is taken to be negligible\cite{NWAInterference}. Therefore, the relativistic Breitâ€“Wigner distribution could be written as:
\begin{equation}
f(q) = \frac{k\pi}{m_{H}\Gamma}\delta(q^2-m_{H}^2)
\end{equation}
where $k$ represents:
\begin{equation}
k=\frac{2\sqrt{2}m_{H}\Gamma\gamma}{\pi\sqrt{m_{H}^2+\gamma}}
\end{equation}
and $\gamma=\sqrt{m_{H}^2(m_{H}^2+\Gamma^2)}$. This is then used to evaluate the cross-section of the Higgs boson production.
\\
\\{\bf Heavy Vector Triplet}
\\
\\Heavy vector bosons are predicted by many new BSM theories with the coupling to quarks, leptons, SM vector bosons and Higgs bosons. To examine the suitable theories, this study attempts to investigate all the couplings with a set-up of one neutral heavy boson, $Z'$, and two degenerate charged bosons, $W'^{\pm}$, with the given coupling constant, $g_{V}$. For optimization,  two models are taken as the benchmarks\cite{Pappadopulo:2014qza,deBlas:2012qp}. Model A is with an additional symmetry breaking to SM, $SU_{1}(2)\times SU_{2}(2) \times U(1) \rightarrow SU_{L}(2) \times U(1)$ giving a weak coupling: $g_{V} \sim \mathcal{O}(1)$. For the scenario of a strong SM boson coupling, the Minimal Composite Higgs Model is taken as model B with the symmetry breaking, $SO(5) \rightarrow SO(4)$ for $4\pi \geq g_{V} \geq 1$. However, because the decay width is proportional to the coupling constant, and the focus of this search is for the narrow resonance, only $6 \geq g_{V} \geq 1$ is considered with $\Gamma_{V'}/m_{V'}$ below $10\%$.
\\
\\To simplify the models, the coupling strength to all fermions are equal with the scale of $g^2c_{F}/g_{V}$ where $g$ is the $SU_{L}(2)$ gauge coupling, and $c_{F}$ is the dimensionless coefficient between bosons and fermions defined as a free parameters of order one (ranged between one and ten) in the phase space of interest. As the fermionic coupling scale is proportional to $1/g_{V}$, model A turns to be more sensitive to the fermionic production with Drell-Yan process, but it is suppressed in model B. In contrast, the coupling to bosons is governed by $c_{H}g_{V}$ with $c_{H}$ as the universal coupling among bosons. Therefore, model B has higher branching ratio of the diboson decay channel and also the production rate from vector boson scattering in this analysis than model A. For the interpretation, the two parameters, $g^2c_{F}/g_{V}$ as well as $c_{H}g_{V}$, construct a two-dimension phase space across which production rates and decay branching ratios vary significantly. 
\\
\\As the coupling to all bosons are the same ($c_{H}g_{V}$), the neutral and charged heavy bosons ($Z'$ and $W'^{\pm}$) have the same decay branching ratio to all SM bosons:
\begin{equation}
BR(Z'\rightarrow ZH) = BR(Z'\rightarrow W^\pm W^\pm) = BR(W'^\pm \rightarrow W^\pm Z) = BR(W'^\pm \rightarrow W^\pm H)
\end{equation}
However, with the small mixing angle (between SM and BSM bosons), the coupling in the transverse component of a field is well suppressed, and the dominant contribution is from the longitudinal component. For the same reason, the couplings to neutral dibosons and $W\gamma$ are also so weak that those channels are ignored in this analysis. For the case of coupling to $HH$, this interaction is forbidden due to the conservation of momentum and angular momentum. 
\\
\\{\bf Randall-Sundrum Graviton}
\\
\\As discussed in Chapter~\ref{Chap:intro}, extra dimensions were proposed as one of the solutions\cite{Randall:1999ee} to the hierarchy problem. It leads to the result that the effective Planck scale, $M_{pl}=2\times10^{18}GeV$, is determined by the existence of extra dimensions from the orginal scale, $M$, and the extra-dimension geometry. The relation between $M_{pl}$ and $M$ is: 
\begin{equation}
\label{Eq:planck_relation}
M_{pl}^{2} = M^{n+2}V_{n}
\end{equation}
where n is the number of dimensions which are not yet observed, and $V$ is the volume constructed from the extra dimensions regardless of the four-dimensional spacetime. Therefore, the visible spacetime is just a manifold under ($4+n$) dimensions.
\\ 
\\Under Randall-Sundrum model, only one more dimension is needed, which hypothesises that the fifth dimension in addition to the spacetime four dimensions is constrained with boundary condition of the $\phi$ periodicity ranged between $-\pi$ to $\pi$ called the ``warped bulk''. It bridges two four-dimensional manifolds at $\phi=\pi$  and $\phi=0$. The ``Hilbert-Einstein''actions under the set-up could be presented as:
\begin{equation}
S = S_{gravity} +S_{obs} + S_{hid}
\end{equation}
\begin{equation}
S_{gravity} = \int d^4x \int^{\pi}_{-\phi}d\phi\sqrt{G}\left[-\Lambda +2M^3R\right]
\end{equation}
\begin{equation}
S_{vis(hid)} = \int d^4x\sqrt{-g_{vis(hid)}}\left[\mathcal{L}_{vis(hid)}-V_{vis(hid)}\right]
\end{equation}
with $\Lambda$ as the cosmological constant, R as the scalar spacetime curvature, and $g$'s are the determinants of metric tensor matrix,  $g_{\mu\nu}$, $V_{vis}$, and $V_{hid}$ are the constant gravitational potentials taken out from the Lagrangian vacuum energy for the visible and hidden spacetimes. After inserting the terms into the Einstein Field Equation, it leads to the solution for the spacetime description in terms of the line element:
\begin{equation}
ds^2=e^{-2\sigma(\phi)}\eta_{\mu\nu}dx^\mu dx^\nu + r_c^2 d\phi^2
\end{equation}
with
\begin{equation}
\sigma(\phi) = kr_c|\phi| \quad k=\sqrt{\frac{-\Lambda}{24M^3}}
\end{equation}
where $\eta$ is the Minkowski metric, and $r_{c}$ is the constant independent of $\phi$ taken as the ``compactification radius'' of the extra dimension on the orbifolding. As a result, the extra dimension only has the dimensional interval, $\pi r_{c}$, at $\phi=\pi$ in the visible spacetime. Taking the space description into Eq. \ref{Eq:planck_relation}, the relation between $r_c$ and $M_{pl}$ could be derived as:
\begin{equation}
M_{pl}^2=\frac{M^3}{k}\left[1-e^{-2kr_{c}\pi}\right]
\end{equation}
This expression indicates that $M_{pl}$ depends on $kr_{c}$, and the weak gravity could be explained with a proper choice of $r_c$. Under the solution, the existence of graviton (the gravitational field) is then taken as the tensor fluctuation on Minkowski metric: $\eta_{\mu\nu} \rightarrow \eta_{\mu\nu}+\bar{h}_{\mu\nu}(x)$. To estimate its mass, the new spacetime geometry is inserted into the Higgs sector in the SM Lagrangian, and it gives the result: $m=e^{-kr_{c}\pi}m_{0}$ with $m_{0}$ as the original mass scale in the visible manifold, and m as the one in the five-dimensional spacetime. (This relation could also be applied to SM particles.) If $e^{kr_{c}\pi}$ is of the order $10^{15}$, the mass scale would be in the scale of $TeV$ under this mechanism, which offers the signature verifiable to the LHC energy scale with the couplings to SM particles derived the same way.
\section{Simulation Samples and Derivation}
Each SM background process and each signal sample are simulated by the procedure mentioned in Sec.~ref{sec:simulation}.  To make a proper comparison between simulation and data, the event numbers are normalised to the theoretical cross section and total data luminosity. However, the modelling of interactions between the ATLAS detector and particles is not perfect, and it leads to discrepancy in efficiency measurements including the particle reconstruction, lepton isolation, trigger, and jet b-tagging efficiency. To recover this disagreement, scaling factors are estimated from the comparison between data and the simulation from a Monte Carlo method (MC) and applied as event weights on the simulation samples. 
\\
\\Another disagreement comes from the inconsistency in distribution of interaction numbers per bunching crossing, $\mu$. To eliminate the effect, one more scale factor is applied through the process called ``pile-up reweighting'' (PRW) to make the simulated $\mu$ distribution agree with data. 
\\
\\After considering all the factors for the data-MC comparison, the final simulation event yield is reweighted to data by:
\begin{equation}
N_{yield} = \mathcal{L}\times \sigma \times \epsilon_{rec} \times \epsilon_{iso} \times \epsilon_{trigger} \times \epsilon_{b-tagging} \times \epsilon_{prw} / N_{mc}
\end{equation}
where $N_{mc}$ is the total event weight from simulation, $\sigma$ is the cross-section for the interaction, and $\epsilon$'s stand for the scaling factors of different contributions like trigger or reconstruction efficiency. $\mathcal{L}$ and $\sigma$ are the integrated luminosity and cross-section of the interaction.
\\
\\
\\{\bf Background Simulation}
\\
\\Some of the SM processes have the same final state to the new physics of our interest: one lepton, one neutrino and multiple jets, and they are called ``irreducible'' background which could not be well-suppressed by selection cuts. This type of backgrounds are estimated from the MC simulation contributed from W+jets ($W\rightarrow l\nu$), $t\bar{t}$ ($t\rightarrow bW \rightarrow bjj$ and $t\rightarrow bW \rightarrow bl\nu$), diboson ($WW/WZ\rightarrow l\nu jj$), Z+jets ($Z\rightarrow ll$), and single top interactions.  
\\
\\The events of W/Z+jets are simulated by \textsc{SHERPA} v2.2.1\cite{sherpa22_twiki}, with the PDF configuration of \textsc{NNPDF30NNLO}\cite{Ball:2012cx} as the baseline generator, and the simulation uncertainty is taken by the comparison to other generators detailed in next chapter. With the complicated process of hadronisation including the broad range of jet $p_{T}$ and involved quark flavours, the simulation is done respectively with multiple slices of $max(h_{T}, p_{T}(W/Z))$ ($h_{T}$ is the scalar sum of $p_{T}$ from all jets) and different number of bottom and charm quarks. The involved matrix element for the simulation are up to 2 partons at NLO (next to leading order) and 4 partons at LO (leading order) which is followed by merging into the Sherpa parton shower. The resulting cross section for normalisation is estimated to NNLO (next next to leading order) of QCD.
\\
\\$t\bar{t}$ events are generated through \textsc{Powheg-Box}\cite{Alioli:2010xd} v2 with the matrix element calculation provided by \textsc{CT10 PDF}\cite{Gao:2013xoa} with the top quark mass set at $172.5~GeV$, and the \textsc{HDAMP} parameters for high $p_{T}$ radiation is set at $1.5m_{t}$. Different from \textsc{SHERPA} as a self-contained generator to do parton shower itself, the simulation from \textsc{Powheg-Box} is then interfaced through \textsc{MadSpin}\cite{Artoisenet:2012st} and \textsc{PYTHIA}8.186 tuned by Perugia 2012 (P2012)\cite{mc11ctunes} and \textsc{CTEQ6L1 PDF}\cite{Stump:2003yu} sets for spin correlation preservation of top quark decays and the following parton shower, fragmentation and underlying events. The renormalisation and factorisation scale of the whole process are determined by $\sqrt{m_{t}^2+p^2_{T}(t)}$. The $t\bar{t}$ cross section used for normalisation is calculated using \textsc{TOP++} 2.0\cite{Czakon:2013goa} with the precision up to NNLO in QCD. To take in the contribution from soft gluon terms, a re-summation with next-to-next-to-leading logarithmic (NNLL) is applied to make further correction.  
\\
\\Single top events are generated through three processes: s-, t- and Wt-channel productions (Feynman diagrams are presented in Fig. \ref{Fig:singletop}). For the simulation of Wt and s-channels, the same recipe from $t\bar{t}$ generation is adopted, while the t-channel one is through \textsc{Powheg-Box} v1 with fixed four-flavour \textsc{CT10f4 PDF}\cite{Lai:2010vv} set but also followed by the same procedure for decay and parton showering from $t\bar{t}$ generation. The renormalisation and factorisation scales are set respectively for the three channels with:
\begin{itemize}
	\item s-channel $\&$ Wt-channel: $m_{t}$
	\item t-channel 4\times$\sqrt{m_{q}^2+p^2_{T}(q)}$ (q is the quark  associated with the single top quark production)
\end{itemize}
The cross section for each production is calculated separately with the description in \cite{Kidonakis:2011wy,Kidonakis:2010ux}.
\begin{figure}[htp]
	\centering
	\subfloat[t-channel]{\includegraphics[width=.25\textwidth]{Chapter3/t-channel.png}}\hfill
	\subfloat[Wt-channel]{\includegraphics[width=.3\textwidth]{Chapter3/Wt-channel.png}}\hfill
	\subfloat[s-channel]{\includegraphics[width=.3\textwidth]{Chapter3/s-channel.png}}
	\caption{The Feynman diagrams of three channels for single top production.}
	\label{Fig:singletop}
\end{figure}

The generation of $WW/WZ$ events are also through \textsc{SHERPA} v2.2.1 for the event production and the hadronisation. 
\\
\\{\bf Signal Simulation}
\\
\\HVT samples are generated via \textsc{MADGRAPH5}\cite{Alwall:2014hca} interfaced to \textsc{PYTHIA8}\cite{Sjostrand:2007gs} with the resonance mass points ranged from $300~GeV$ to $5~TeV$ with $100~GeV$ spacing. For simplicity, $g_V=1$ and $g_V=3$ are set for model A and model B respectively.
\\
\\RS graviton events are also simulated through \textsc{MADGRAPH5} and \textsc{PYTHIA8}, and only the ggF production is considered for this signal. Within the simulation, $r_{c}=1$ is set as the default for the simulation, but it is also reweighted in the resonance mass distribution at parton level for $r_{c}=0.5$ \footnote{Reweighting is a feature of Madgraph. For similar configurations of simulations, the samples can be obtained by altering the weight of existing samples based on the kinemtic properties.} This is for the comparison with the result from the CMS collaboration. The decay width of graviton mass with this configuration is expected to be $\approx6\%$. The decay widths and cross-sections of HVT and RS graviton are summarised in tab.~\ref{Tab:xs_decaywidth}.
\\
\\For the NWA Higgs boson, its interference to the SM Higgs boson ($125~GeV$) is assumed to be negligible as discussed in \ref{sec:signal_intro}. Its narrow decay width is set as a constant at $4.07~MeV$ for all mass points which is beyond the experimental resolution with the production of ggF and VBF, which are simulated separately. The simulation is done by \textsc{Powheg-Box} v2 showered with \textsc{PYTHIA8} under \textsc{CTEQ6L1} PDF set. 
\begin{table}[htb]
	\caption{The decay width and cross section of HVT and RSG at $800~GeV$, $1.6~TeV$, and $2.4~TeV$ mass points}
	\centering
		\begin{tabular}{|c|ccc|cc|}
          \hline
          \hline
                   & \multicolumn{3}{c|}{ HVT W' and Z' }                                     & \multicolumn{2}{c|}{ RS $G*$}  \\
              $m$  & $\Gamma$ & $\sigma \times BR(Z' \to WW)$ & $\sigma \times BR(W' \to WZ)$ & $\Gamma$ & $\sigma \times BR(G* \to WW)$ \\
            $[TeV]$& $[GeV]$  & $[fb]$                        & $[fb]$                        & $[GeV]$  & $[fb]$      \\
          \hline
               0.8 & 32       & 354                           & 682                           & 46       & 301   \\
               1.6 & 51       & 38.5                          & 79.3                          & 96       & 4.4 \\
               2.4 & 74       & 4.87                          & 10.6                          & 148      & 0.28 \\
          \hline
         \end{tabular}
	\label{Tab:xs_decaywidth}
\end{table}
\noindent
\\
\\{\bf Derivation}
\\
\\For practical reasons, the analyses were not run on AODs directly. Instead, they went through the ``derivation'' procedure composed of ``trimming'' and ``slimming'' to drop down variables and events of no interest first\cite{Borodin:2015wqa}, which outputs the data format called derived AOD (DAOD). For the broad variety of analysis types, a couple of derivation schemes are applied, and the analyses with similar final states share the same derivation scheme. 
\\
\\With the final state of this analysis, ``HIGG5D2'' is chosen with the derivation scheme as the following:
\begin{itemize}
  \item trigger: passing at least one unprescaled electron, OR muon, OR $\met$ trigger
  \item lepton: one electron OR muon with $p_{T}>15~GeV$ 
  \item jet: two small R jets with $p_{T}>20~GeV$, OR one  small R jet with $p_{T}>100~GeV$, or one large R jet with $p_{T}>150~GeV$
\end{itemize}
\section{Physical Object Definition}
\label{sec:obj_def}
Because the LHC is using protons as the beam source, it leads to the enormous production of hadronic jets. Within the environment, most reconstructed objects have the potential to suffer from great contamination from jet misidentified as other objects. Therefore, the definition on the signal objects selection as well as the loose object rejection for this analysis is to keep the signal efficiency and significant suppression of misidentification of the intended objects at the same time. 
\\
\\{\bf Electron}
\\
\\The electrons in this analysis are defined as two types, loose and signal, and each event only has exactly one signal lepton without additional loose one. Signal electrons are required to have $p_{T}$ above $27~GeV$ to reach the trigger efficiency turn-on plateau, and $|\eta|<2.47$ is applied on both electron types within the acceptance of inner detector with the crate region vetoed ($1.37<|\eta|<1.52$). The impact parameter cut is required to suppress the electrons contributed by pile-up events by the cuts on the distance between the electron and the primary vertex on the transverse plan and the z-axis. The selection criteria for signal and loose electrons are shown in Tab. \ref{Tab:eledefin}. 
\\
\\In addition to the fundamental quality requirement, the overlap removal is applied afterwards to prevent the objects reconstructed from the same detector signature. When an electron shares inner detector tracks with any muon candidate, the electron is discarded. The existence of a nearby jet defined by:
\begin{itemize}
	\item $0.2<\Delta R(e,j)<min(0.4,0.04+10~GeV/p_{T}(e))$
\end{itemize}
also makes the electron removed. The final requirement on electron is that it shall be consistent with the trigger level electron which fired the required electron trigger to suppress the QCD background. 
\begin{table}[htb]
	\caption{Selection for electron candidates used in the analysis. Loose and signal electrons are defined. }\label{Tab:eledefin}
	\centering
	\begin{tabular}{|c||c|c|}
		\hline
		& \multicolumn{2}{c|}{ Electrons}\\
		\cline{2-3}
		&   Loose & Signal \\
		\hline
		$p_T$ & $>7~GeV$ & $>27~GeV$  \\
		\hline
		$| \eta |$ &  \multicolumn{2}{c|}{ $< 2.47 ~ \notin [1.37,1.52]$ } \\
		\hline
		Identification & LooseLH & TightLH   \\
		\hline
		Isolation       &   LooseTrackOnly & FixedCutTight  \\
		\hline
		$|d_0/\sigma(d_0)^{BL}|$ &   \multicolumn{2}{|c|}{  < 5}  \\
		\hline
		$|z_0\sin\theta| $  & \multicolumn{2}{|c|}{< 0.5~mm}  \\
		\hline
	\end{tabular}
\end{table}
\noindent
\\{\bf Muon}
\\
\\Similar to electrons, loose and signal muons are defined with $p_{T}$ and $|\eta|$ cuts in the consideration of trigger turn-on curve plateau and inner detector coverage. The requirement on muon impact parameters is tightened for better rejection to the cosmic muons. The selection criteria are shown below in Tab. \ref{Tab:mudefin}
\\
\\Different from the electrons, muons are kept, when it is close to small-R (R=0.4) jets. And, those jets are discarded, if they fulfil either of the following conditions:
\begin{itemize}
  \item $\Delta R(\mu,j)<0.2$
  \item number of associated tracks is smaller than 2 (the ghost-association algorithm mentioned in large-R jet definition in the following)
  \item $p_{T}^{\mu}/p_{T}^j<0.5$ with $\Delta R(\mu,j)<min(0.4,0.04+10~GeV/p_{T}(\mu))$
  \item $p_{T}^{\mu}/\sum^{n}_{1} p_{T}^{trk}<0.7$ with $\Delta R(\mu,j)<min(0.4,0.04+10~GeV/p_{T}(\mu))$
\end{itemize}
\noindent
If a jet pass the above selection with a distance po the muon with $\Delta R(\mu,j)<min(0.4,0.04+10~GeV/p_{T}(\mu))$, the muon is discarded instead.  
\\
\\The last selection in muon is that it shall be spatially consistent to the trigger muon if muon trigger is fired in the event. 
\begin{table}[htb]
	\caption{Selection for muon candidates used in the analysis. Veto and signal electrons are defined.}\label{Tab:mudefin}
	\centering
	\begin{tabular}{|c||c|c|}
   \hline
& \multicolumn{2}{c|}{Muons}\\
\cline{2-3}
&  Loose & Signal  \\
\hline
$p_T$ threshold &  7~GeV & 27~GeV  \\
\hline
$| \eta |$      &  $< 2.7$ & $< 2.5$   \\
\hline
Identification  &  Loose & Medium  \\
\hline
%Track Quality   &  - & - & ''Loose Muon'' & ''Loose Muon'' \\
Isolation       &   LooseTrackOnly & FixedCutTightTrackOnly  \\
\hline
$|d_0/\sigma(d_0)| w.r.t. BL$ &   \multicolumn{2}{|c|}{< 3} \\
\hline
$|z_0\sin\theta| $ &   \multicolumn{2}{|c|}{< 0.5~mm} \\
\hline
	\end{tabular}
\end{table}
\noindent
\\{\bf Small R Jets [R=0.4]}
\\
\\In the intended final states, the jets (denoted as $j$) come from the decay of W bosons ($W\to jj$) or the remnant quarks from the vector boson fusion ($jj\to WWjj$ or $jj \to WZjj$). Because of the kinematic properties, the two types of jets are selected respectively. The full selection criteria are in Tab. \ref{tab:sjdefinit}.
\\
\\The pair of VBF jets are supposed to be a high mass dijet system with wide separation, so they have a tighter $p_{T}$ selection of $p_{T}>30~GeV$ but a looser $|\eta|$ cut, $|\eta|<4.5$. For signal jets (the jets from the boson decay), they are only required to have $p_{T}>20~GeV$, and only the ones within the acceptance of inner detector ($|\eta|<2.5$) are taken as jet candidates for event selection. The jet quality requirement is to remove the ``fake jets'' from calorimeter noise pulse, cosmic ray, or non-collision background (like beam-halo), which is called ``jet cleaning''\cite{ATLAS:2010vza}.

\begin{table}[tbh]
	\caption{Selection for small-R jets}\label{tab:sjdefinit}
	\vspace{2.0em}
	\centering
	\begin{tabular}{|c||c|c|}
		\hline
		             & \multicolumn{2}{|c|}{ Small-R Jets }\\
		\hline
		             & Signal Jets & VBF Jets \\
		\hline
		Algorithm    & \multicolumn{2}{|c|}{ anti$-k_t$, $R=0.4$}\\
		\hline
		$p_T$        & $>20~GeV$ & $>30~GeV$\\
		\hline
		|$\eta$|     & $< 2.5$ & $<4.5$  \\
		\hline
		Quality      & \multicolumn{2}{|c|}{not ``bad'' jet}\\
		\hline
		JVT          & \multicolumn{2}{|c|}{$< 0.59$ ( $| \eta | < 2.4 ~ \& \& ~p_T < 60 $ GeV)} \\
		\hline
		b-Tagging    & \multicolumn{2}{|c|}{\texttt{MV2c10}, 85\% efficiency} \\
		\hline
	\end{tabular}
\end{table}
\noindent
{\bf Large R Jets [R=1.0]}
\\
\\When the W or Z boson is highly boosted and decayed from a heavy particle, the outcoming quarks would be close to each other. In this case the small R jets would not have enough resolution power to reconstruct them individually, so the large R jets (or called ``fat jets'' and denoted as $J$) are reconstructed to collect the energy deposits from the close-by quarks. The full selection on the fat jets could be seen in Tab. \ref{Tab:Jdefinit}. With this topology, the jet mass and $p_{T}$ would need a further correction due to the limited calorimeter spatial resolution. This is performed with the track-assisted mass, $m^{TA}$\cite{ATLAS-CONF-2016-035}, as the calorimeter cannot provide enough spatial resolution. $m^{TA}$ is estimated from the tracks left by charged jet partons inside the fat jets defined as:
\begin{equation}
m^{TA} = m^{trk} \times \frac{p_{T}^{J}}{\sum p_{T}^{trk}}
\end{equation}  
Here, $m^{trk}$ is the reconstructed mass of the tracks taken as massless particles, and $p_{T}^{trk}$ is the vector sum from $p_{T}$ of tracks.  The ratio of $p_{T}$ between the jet and tracks is to take in the neutral-to-charge fluctuations. It could then be combined with the calorimeter mass, $m^{calo}$, into the combined mass, $m^{comb}$, by this definition:
\begin{equation}
m^{comb} = \frac{\sigma_{{calo}}^{-2} m^{{calo}} + \sigma_{{TA}}^{-2} m^{{TA}} }{\sigma_{{calo}}^{-2} + \sigma_{{TA}}^{-2}}
\end{equation}
with $\sigma_{{calo}}$ and $\sigma_{{TA}}$ as pre-estimated mass resolutions for  the calorimeter and track-assisted mass which are assumed to be uncorrelated. From Fig. \ref{Fig:combinedmassperformance}, it could be seen that the calorimeter mass has better performance in the low $p_{T}(W)$ regime benefited from the great energy resolution, but it is degraded as $p_{T}(W)$ increases, while the track-assisted mass performed in an opposite way. The combined mass takes the merits of these two mass definitions and provide the best mass resolution ($\sim10\% (15\%)$ at jet $p_{T}=1~TeV(2.5~TeV)$). It is taken as the nominal fat jet mass in this analysis with the selection of $m^{comb}>50~GeV$. The jet $p_{T}$ is the corrected by $p_{T}^{comb}=p_{T}^{calo}\times m^{comb}/m^{calo}$ 

\begin{table}[h]
	\caption{Selection for large-R jets}\label{Tab:Jdefinit}
	\vspace{2.0em}
	\centering
	\begin{tabular}{|c||c|}
		\hline
		& Signal Large-R Jets\\
		\hline
		Algorithm & anti$-k_t$, $R=1.0$\\
		$p_{T}$   & >200~GeV\\
		$| \eta |$      & $< 2.0 $\\
		Mass threshold  & 50~GeV\\
		W/Z Tagger &  $D^{\beta =1}_2 \& m^{comb}$ \\
		\hline
	\end{tabular}
\end{table}
\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=0.6\hsize]{Chapter3/mass_resolution}
		\caption{The jet mass resolution as a function of jet $\p_{T}$ for jets produced from boosted $W$ boson\cite{ATLAS-CONF-2016-035}. Three different jet mass reconstruction algorithms are displayed: the calo-jet mass ($m^{{calo}}$), the track-assisted mass ($m^{{TA}}$), and the combined TA+calo mass ($m^{{comb}}$).}
		\label{Fig:combinedmassperformance}
	\end{center}
\end{figure}
\noindent
However, the combined mass is still not proficient to select the W/Z decayed fat jets precisely, so the substructure of jets is needed to improve the boson tagging. This extra information is extracted with the subjets of $R=0.2$ from a $k_{T}$ algorithm performed on the clusters used to reconstruct small-R jets. Those tiny jets are then taken as the new entities to be ``ghost-associated'' with the fat jets, which mean the $anti-k_{T}$ algorithm is performed on the $R=0.2$ subjects $p_{T}$ selected by the threshould, $p_{T}^{R=0.2}/p_{T}^{R=1.0}>0.05$, for a re-clustering. The jet substructure information could then be given by the discriminant, $D^{\beta =1}_{2}$, for the W/Z boson recognition\cite{Larkoski:2014gra} which is defined as:
\begin{equation}
D^{\beta =1}_{2} = \frac{e^{\beta}_{3}}{e^{\beta}_2} 
\end{equation}
with $e^{\beta}_{2}$ and $e^{\beta}_{3}$ as:
\begin{equation}
e^{\beta}_{2} = \frac{1}{(p_{T}^{jet})^2}\displaystyle\sum\limits_{i<j\in J}p_{T}^{i}p_{T}^j(\Delta R_{ij})^{\beta}
\end{equation}
\begin{equation}
e^{\beta}_{3} = \frac{1}{(p_{T}^{jet})^3}\displaystyle\sum\limits_{i<j<k\in J}p_{T}^{i}p_{T}^{j}p_{T}^{k}(\Delta R_{ij}\Delta R_{jk}\Delta R_{ik})^{\beta}
\end{equation}
where $\Delta R$ is the distance between two $R=0.2$ jets with i, j, and k as the indices for subjets. The boson tagging requirement is then done by a 2D cut on both $D^{\beta =1}_{2}$ and $m^{comb}$ as a function of $p_{T}$ shown in Fig. \ref{Fig:newWZtaggerWP} with two working points (WPs), $50\%$ and $80\%$, for the tagging efficiency. The curves are smoothed, but the cuts are applied in different bins.
\begin{figure}[ht]
	\begin{center}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/MassWindow_newWZTagger}
		}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/D2UpperCut_newWZTagger}
		}
		\caption{The thresholds of the mass window cut ($m^{combo}$) (a) and the upper cut on $D^{\beta =1}_2$ (b) as a function of $p_{T}$ used in this analysis. The cuts  for $W$-($Z$)-boson tagging is shown by red (blue) lines.}
		\label{Fig:newWZtaggerWP}
	\end{center}
\end{figure}
\noindent
\\
\\{\bf Missing Transverse Energy}
\\
\\Although $E^{missing}_{T}$ is supposed to be reconstructed as shown in Sec. \ref{sec:obj rec}, hadronically decayed taus and photons are treated as jets for the intended final state in this analysis.
\\
\\The cut on $E^{miss}_{T}$ will be discussed in the next section.   
\section{Event Selection}
The event selection in this analysis is performed to define signal regions which is expected to be enriched with events of new-physics-like (signal-like) detector signatures as well as the control regions enriched with the SM events which is to give constrains in the SM background contributions in signal regions when the simultaneous fit is performed(details in the next chapter). For this analysis, two control regions are defined corresponding to two dominant background, W+jets (WR) and $t\bar{t}$ (TR) interactions, whose events are orthogonal to each other.  
\\
\\
\\As discussed before, the signal regions are defined into four categories by production channels (VBF and ggF/DY) and jet topologies (single merged jets or two resolved jets): VBF merged, VBF resolved, ggF/DY merged, and ggF/DY resolved, and they are defined as the following:
\begin{itemize}
	\item VBF: the existence of a jet pair with high invariable mass and broad $\eta$ separation \\(the selection criteria is in the following) 
	\item ggF/DY: no VBF jet pair is found
	\item Merged: at lease one large-R jet is found fulfilling the definition mentioned in Sec.~\ref{sec:obj_def}.
	\item Resolved: no fat jet is found. 
\end{itemize}
By this definition, the four categories are orthogonal to each other, and the category priority is given as:
VBF merged $\to$ VBF resolved $\to$ ggF merged $\to$ ggF resolved. This scheme is performed to give the category with less background contamination higher priority. Furthermore, each categories have WW and WZ signal regions due to different charges of signal particles ($W'\to WZ$, and $G/H/Z' \to WW$) which are selected by the mass of hadronically decayed bosons. For WW signal regions, the fat jet or the dijet system should have the mass in the W mass window, while they are in the Z mass window for the WZ signal region. (It should be noted that WW and WZ signal regions are not orthogonal to each other).  
\\
\\If a event fails the signal region selection and passes either  the control region selections, it would be recycled into the control regions. Then, the same event categorisation is performed in control regions , which gives each signal category a corresponding control region. The priority of event categorisation for both signal and control could be seen in Fig.~\ref{Fig:order}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\hsize]{Chapter3/order}
	\caption{Illustration on the priority of event categorisation for both signal and control regions.}
	\label{Fig:order}
\end{figure}

\subsection{Trigger}
\label{Subsec:Trigger_resonance}
The first applied criterion on event selection is the trigger. The recorded data is a broad collection of different physical signatures, and our final state only accounts for a small fraction of them, so the trigger is used to choose the events with signatures of interest. Hereby, the chosen triggers for this analysis are single electron triggers, single muon triggers, and $\met$ triggers. Because of the increasing luminosity provided by LHC, the trigger thresholds were enhanced in 2016 to reduce the trigger rate. For the MC samples, the run number is randomly generated, and the events shall only pass the triggers available in the random run number. The full trigger set used in this analysis is shown in Tab. \ref{tab:triggers}
\begin{table}[h]
	\caption{The list of triggers used in the analysis. } \label{tab:triggers}
	
		\footnotesize
		\begin{center}
		\begin{adjustbox}{center}
			\begin{tabular}{|l|c|c|c|}
				\hline
				\multirow{2}{*}{Data-taking period} & \multirow{2}{*}{Electron channel} & \multicolumn{2}{c|}{ Muon channel }  \\
				\cline{3-4}
				& & $p_{T},\left(\mu\nu\right) < 150\,GeV$ & $P_{T},\left(\mu\nu\right) > 150\,GeV$   \\
				\hline
				\multirow{3}{*}{\centering {2015}} & HLT\_e24\_lhmedium\_L1EM20 & HLT\_mu20\_iloose\_L1MU15 & \multirow{3}{*}{ HLT\_xe70 } \\
				& HLT\_e60\_lhmedium  & HLT\_mu50 & \\
				& HLT\_e120\_lhloose & & \\
				\hline
				\multirow{2}{*}{\centering {2016a (run $< 302919$)}} & HLT\_e26\_lhtight\_nod0\_ivarloose & HLT\_mu26\_ivarmedium  & \multirow{3}{*}{ HLT\_xe90\_mht\_L1XE50 } \\
				& HLT\_e60\_lhmedium\_nod0 & HLT\_mu50 &  \\
				($L<1.0\times10^{34}\,{ cm}^{-2}\,{s}^{-1}$) & HLT\_e140\_lhloose\_nod0 & & \\
				\hline
				{\centering {2016b (run $\geq 302919$)}} & \multirow{2}{*}{same as above} & \multirow{2}{*}{same as above}  &  \multirow{2}{*}{HLT\_xe110\_mht\_L1XE50} \\
				($L<1.7\times10^{34}\,{ cm}^{-2}\,{ s}^{-1}$) & & &\\
				\hline
				\hline
				Total int. lumi. [$fb^{-1}$] &  36.1 & 35.6 & 35.9 \\
				\hline
			\end{tabular}
		\end{adjustbox}
		\end{center}
	
\end{table}
\noindent
\\Three electron triggers are used in electron channel including the unprescaled lowest threshold one to maximize the signal efficiency. The other two triggers are used to select high $\p_{T}$ electrons with looser isolation requirement. The combined performance of the triggers is around $90\%$ efficiency at the turn-on plateau as a function of $p_{T}$, which was studied by a dedicated $Z\to ee$ sample\cite{Aad:2019wsl}.
\\
\\In muon channel, $\met$ and muons triggers are both used, which depends on the transverse momentum sum of muon and $\met$ denoted as $p_{T}(\mu\nu)$. For the scenario of $p_{T}(\mu\nu)<150~GeV$, two unprescaled single muon triggers are used with the logic of ``OR'': one for single isolated muons passing the lowest threshold, and another one with for single muon passing a higher threshold without the requirement on isolation. Otherwise, $\met$ triggers are chosen for events with $p_{T}(\mu\nu)>150~GeV$, because the combined performance of muon triggers can only reach $70\%$ efficiency on the plateau with the study on signal samples.
\\
\\However, the $\met$ cut in this analysis is below the plateau, so there might be the inconsistency between data and simulation in terms of the efficiency. Therefore, a ``tag and probe'' method is applied to study the trigger efficiency as a function of $p_{T}(\mu\nu)$ (because muons are invisible in the L1Calo system, so $\met$ in the hardware trigger level is actually $p_{T}(\mu\nu)$). This study is performed on boosted and resolved channels respectively. The tagged events are required to fulfil the following conditions for the resolved (boosted) channel:
\begin{itemize}
	\item[] a) one muon with $p_{T}>27~GeV$
	\item[] b) at least 2 signal jets (1 fat jet) selected for resolved (boosted) category
	\item[] c) the unprescaled lowest threshold muon trigger is fired
	\item[] d) $p_{T}(\mu\nu)>75GeV$
\end{itemize}
Then, the probe is performed by the selection on the tagged events by whether the events pass the $\met$ trigger. The efficiency is therefore taken as the ratio of probed event number over the tagged event number. The result for data and simulated $t\bar{t}$ events are shown in Fig.~\ref{Fig:eff_met_merged} for boosted channel and Fig.~\ref{Fig:eff_met_resolved} for the resolved channel. They are presented as a function of $p_{T}(\ell\nu)$, and it could be seen that the efficiency reaches the $100\%$ plateau at $pT(\mu\nu)\sim200~GeV$. However, $E^{miss}_{T}$ trigger is applied for the case of $p_{T}(\mu\nu)>150~GeV$, and there is the efficiency discrepancy between data and simulation sample. To recover the discrepancy, the scaling factor is applied as an extra weight on simulated events for a proper modelling of background estimation from simulation. In the concern of the orthogonality, signal regions are a small fraction of events used in this study, so the interference is negligible. 
\begin{figure}[ht]
	\begin{center}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/Merged_TrigEff_ptW_data.eps}
		}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/Merged_TrigEff_ptW_ttbar.eps}
		}
		\caption{The upper plot is $p_{T}(\mu\nu)$ distribution of tagged (real) and probed (dotted) events in boosted channel for data (a) and $t\bar{t}$ events (b). The lower plots is the efficiency as a function of $p_{T}(\mu\nu)$}
		\label{Fig:eff_met_merged}
	\end{center}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/Resolved_TrigEff_ptW_data.eps}
		}
		\subfloat[]{
			\includegraphics[width=0.48\hsize]{Chapter3/Resolved_TrigEff_ptW_ttbar.eps}
		}
		\caption{The upper plot is $p_{T}(\mu\nu)$ distribution of tagged (real) and probed (dotted) events in resolved channel for data (a) and $t\bar{t}$ events (b). The lower plots is the efficiency as a function of $p_{T}(\mu\nu)$}
		\label{Fig:eff_met_resolved}
	\end{center}
\end{figure}
\noindent
\subsection{Event Cleaning and Preselection}
After the trigger, the event ``quality'' is verified by a series of flags in data determining the suitability of an event for physical analyses. The following is the list:
\begin{itemize}
	\item {\bf Good Run}: when the detector operates in a proper status without intolerable defects like dead channels in the colorimeter covering a significant region, the runs go into the good run list (GRL). Only the events contained in the GRL are considered in this analysis. 
	\item {\bf Primary Vertex}: because all the physical objects are required to origin from the primary vertex, its existence is essential. Events without a proper primary vertex (defined in Sec. \ref{sec:obj rec}) are discarded.
	\item {\bf Tile Error Veto}: part of the channels in tile detector are broken. If they accept any physical objects, this flag would be marked, and the events are vetoed.
	\item {\bf LAr Error Veto}: part of the channels in LAr detector are broken. If they accept any physical objects, this flag would be marked, and the events are vetoed.
	\item {\bf SCT Error Veto}: part of the channels in SCT detector are broken. If they accept any physical objects, this flag would be marked, and the events are vetoed.
	\item {\bf Core Error Veto}: during data-taking periods, the ATLAS central DAQ system might suffer from some glitches which broke the data recording, and the flag is marked for events. They are also vetoed in this analysis. 
\end{itemize}
\subsection{Reconstructed Mass of Signal Objects, $m_{WV}$}
\label{Subsec:mwv}
This analysis is searching for the mass resonance of exotic particles. If there is the existence of signal particles with the narrow width assumption applied in this analysis, a peak should be spotted in the mass spectrum of reconstructed from signal objects (two jets, one lepton, and one neutrino) denoted as $m_{WV}$.  Therefore, $m_{WV}$ is taken as the discriminant to input to the statistic interpretation which will be discussed in the next chapter. However, the longitudinal momenta of neutrinos, $p_{z}^\nu$, are not measured, so the mass resolution is degraded. To improve it, $p_{z}^{\nu}$ is solved with the assumption that all neutrinos are coming from the process, $W\rightarrow l\nu$, so $\met$ could be taken equally as transverse component of neutrino momentum, $p_{T}^\nu$. Therefore, the equation of energy conservation of $W$ boson decays can be written down as:
\begin{equation}
m^2_W = m_{l}^{2} + 2E^{l}\sqrt{ {p_{T}^\nu}^2 + {p_{z}^\nu}^{2} }  - 2 \vec{p}_{T}^l \cdot \vec{p}_{T}^\nu - 2 {p_{z}^l}{p_{z}^\nu}
\end{equation}
with the four-vector of leptons as $(m_{l}, p_{T}^l, p_{z}^l)$ where $m_{l}$ and $p_{T}^l$ could be written as:
\begin{equation}
p_{T}^l=\sqrt{{p_{x}^l}^{2}+{p_{y}^l}^{2} }
\end{equation}
\noindent
\begin{equation}
m_{l} =\sqrt{{E^l}^2-{p^l_T}^2-{p^l_z}^2}
\end{equation}
In SM, W bosons have the mass of $80~GeV$ ($m_W$), so $m_{l}$ for electrons and muons is negligible. This leads to the quadratic equation with $p_{z}^\nu$:
\begin{equation}
4{p_T^l}^{2} {p_T^\nu}^{2} - 4 \left( m_{W}^{2} + 2 \vec{p}_{T}^l \cdot \vec{p}_{T}^\nu \right) p_{z}^l p_z^\nu - \left( m_{W}^{2} + 2\vec{p}_{T}^l \cdot \vec{p}_{T}^\nu  \right)^{2} +4{p_T^l}^{2} {p_{T}^\nu}^{2} = 0
\end{equation}
\noindent
If the solutions are complex, only the real terms are taken into this analysis, and the imaginary term is discarded. If both solutions are real, the resolutions are compared with the absolute value of solutions (bigger one and smaller ones) to determine which solution to use. It is defined as:
\begin{equation}
\sigma = \frac{p_z^{truth}-p_z^{\nu}}{p_z^{truth}}
\end{equation}
\noindent
with $p_z^{truth}$ as the neutrino longitudinal momentum at generator level (MC truth). The result could be seen in Fig. \ref{Fig:netrinoPz}, and it indicates the bigger one has slightly better performance in terms of the mass resolution, so it is kept.  
\begin{figure}
	\centering
    \includegraphics[width=0.5\hsize]{Chapter3/neutrinoPz}
    \caption{The $p_{z}^\nu$ resolution with absolute values of the solutions, bigger (blue) and smaller (red) one. }
    \label{Fig:netrinoPz}
\end{figure}
\noindent
The similar assumption is also made on the hadronically decayed objects. The two selected signal jets are also assumed to always originate from a W or Z boson, so the reconstructed mass of two selected signal jets, $m_V$, is supposed to be $sim$80~GeV ($sim$91~GeV) for W (Z) boson. Therefore, the invariant mass and transverse momentum of this di-jet system, $m_{jj}$ and $p_T(j,j)$, are taken to correct the transverse momentum of the di-jet system, $p_{T}^{corr}$:
\begin{equation}
p_{T}^{corr}=p_{T}(j,j)\times \frac{m_{V}}{m_{jj}}
\end{equation}
With the two corrected boson systems ($W\to \ell\nu$ with $p_{z}^{\nu}$ correction and $m_W$, and $V\to jj$ with $p_{T}^{corr}$ and $m_V$), the invariant mass of the diboson system, $m_{WV}$, is evaluated. A comparison for the evaluations with and without this ``mass-constraint'' correction could be seen in Fig. \ref{Fig:WHadmassConst} with NWA Higgs boson samples of 300~GeV, 500~GeV, and 700~GeV, and the ones with this correction has a better resolution.  
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\hsize]{Chapter3/lvjjmass_WmassConstraint.png}
	\caption{$m_{WV}$ distributions for $gg \rightarrow H \rightarrow WW$ signals at $m=300~GeV$ (solid), 500~GeV (dashed) and 700~GeV (dot), with (red) and without (blue) $W$-mass constraint to $W \rightarrow jj$ system.}\label{Fig:WHadmassConst}
\end{figure}
\noindent
\subsection{VBF Event Selection}
As VBF signal regions have better sensitivity than ggF/DY ones, the selection criteria play an important role in this analysis. The optimization on the selection is conducted in three steps. First, all VBF events are required to have at least 4(2) jets in the resolved (boosted) channel. Second, the two jets in the dijet system are chosen as the pair with the highest mass, opposite $\eta$ signs, and not b-tagged. This pair was chosen prior to the $W/Z\rightarrow jj$ signal jet selection and removed from signal jet candidates. Then, the optimization is performed on a 2-dimensional phase space constructed by $\Delta\eta(j,j)$ and $m(j,j)$ which are two most evident signatures of this production process. The performance of the cuts on the two variables is determined by signal significance, $\sigma$, which is evaluated on the $m_{WV}$ distribution which is defined as:
\begin{equation}
\sigma^2=\displaystyle\sum^{N^{bin}}_i\left(\frac{s_i}{s_i+b_i+({\Delta b_i})^2}\right)^2
\end{equation}
Here, $s_i$ and $b_i$ are the signal and background event numbers in the $m_WV$ distribution with the binnings shown in Eq.~\ref{Eq:boosted_mbin} and \ref{Eq:resolved_mbin}, and $\Delta$ is set at $1.5\%$ taken as the hypothesised systematic uncertainty. Fig. \ref{Fig:VBFOptimization} shows the result of the optimization performed on the signal sample with $700~GeV$ HVT, and the best significance could be achieved by:
\begin{itemize}
	\item {\bf $m_{jj}^{VBF}> 770~GeV$}
	\item {\bf $\Delta\eta(jj)>4.7$}
\end{itemize}
The other reason to choose this set of cuts is to make it consistent with $WZ/ZZ \rightarrow lljj/\nu\nu jj$ analysis\cite{EXOT-2016-29} for the combination in next chapter. In the following of this thesis, the reconstructed mass of two VBF jets is denoted as $m^{VBF}$. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{Chapter3/VBF700_SignfSpace}
	\caption{The signal significance for the VBF $WW$ signal as a function of the VBF cuts on $\Delta \eta(j_1,j_2)$ (which is shown as DY(jj) in the plot) and $m(jj)$ for signal mass 700~GeV respectively. The black outlined bins are those whose values vary from the maximum by less than 5\%.}
	\label{Fig:VBFOptimization}
\end{figure}
\input{TexFiles/ResonEvtSel}
\input{TexFiles/MJModel}